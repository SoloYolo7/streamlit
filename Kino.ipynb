{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uE2QE_oGXy_O6OX1gUS0FB6TtwSQNkXg",
      "authorship_tag": "ABX9TyOC4vZdy2v3KRypwv1uolpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoloYolo7/streamlit/blob/main/Kino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNNgho9lctUP",
        "outputId": "f778bb04-9ebd-4165-a992-b4fe236e829c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Монтируйте Google Диск к Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Задайте путь к файлу на вашем Google Диске\n",
        "file_path = \"/content/drive/MyDrive/kinopoisk.jsonl\"\n",
        "\n",
        "# Вы можете использовать переменную file_path для обращения к файлу\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the dataset\n",
        "data = []\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Display the first few entries to understand the dataset structure\n",
        "data[:5]"
      ],
      "metadata": {
        "id": "Ks1DRsmG5b0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG5fyFCf5_sB",
        "outputId": "5a51049a-231a-4434-dbbb-007b20562c54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Загрузка данных\n",
        "file_path = '/content/drive/MyDrive/kinopoisk.jsonl'  # Укажите правильный путь к файлу\n",
        "data = pd.read_json(file_path, lines=True)\n",
        "\n",
        "# Функция для предобработки текста\n",
        "def preprocess_text(text):\n",
        "    # Удаление лишних символов\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "    text = text.lower()  # Приведение к нижнему регистру\n",
        "\n",
        "    # Лемматизация\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "    tokens = text.split()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('russian')]\n",
        "    text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Предобработка текста\n",
        "X = data['content'].apply(preprocess_text)\n",
        "y = data['grade3']\n",
        "\n",
        "# Разделение на обучающую и валидационную выборки\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Векторизация текста с использованием TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "\n",
        "# Теперь данные готовы к обучению модели\n",
        "\n"
      ],
      "metadata": {
        "id": "O5E6tUUV5wYI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Создание экземпляра SMOTE и балансировка обучающего набора\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Проверка нового распределения классов\n",
        "from collections import Counter\n",
        "print(\"Распределение классов после SMOTE:\", Counter(y_train_smote))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXdeAJZW6ZrL",
        "outputId": "eb4d8148-3b9e-4bb5-d654-644dd2624b58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Распределение классов после SMOTE: Counter({'Good': 21810, 'Bad': 21810, 'Neutral': 21810})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Создаем новый DataFrame с балансированными данными\n",
        "balanced_df = pd.DataFrame({'X_train_smote': X_train_smote, 'y_train_smote': y_train_smote})\n",
        "\n",
        "# Указываем путь и имя файла, в который вы хотите сохранить сбалансированные данные\n",
        "output_file = \"сбалансированные_данные.csv\"\n",
        "\n",
        "# Сохраняем DataFrame в файл CSV\n",
        "balanced_df.to_csv(output_file, index=False, sep=';')  # Вы можете выбрать другой разделитель, если нужно\n"
      ],
      "metadata": {
        "id": "y7DqZtg2Fl02"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "# Обучение логистической регрессии\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Измерение времени предсказания на валидационном наборе\n",
        "start_time = time.time()\n",
        "y_val_pred = log_reg.predict(X_val_tfidf)\n",
        "end_time = time.time()\n",
        "prediction_time = end_time - start_time\n",
        "\n",
        "# Вычисление F1-метрики\n",
        "\n",
        "f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "\n",
        "print(\"Время, затраченное на предсказание:\", prediction_time)\n",
        "print(\"F1-Score для логистической регрессии:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQnvncolGH1e",
        "outputId": "66d8c878-3425-4260-a293-935a3188601d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Время, затраченное на предсказание: 0.0077648162841796875\n",
            "F1-Score для логистической регрессии: 0.6500895701920437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Сохранение весов модели в файл\n",
        "model_weights_filename = \"/content/drive/MyDrive/logistic_regression_weights.pkl\"\n",
        "joblib.dump(log_reg, model_weights_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB3VNJQUDG09",
        "outputId": "0cbd41be-be99-416e-a274-273a5434179d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/logistic_regression_weights.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}